<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="WEIQI JIANG">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="数据分析师招聘信息爬取及分析"/>
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="不会游泳的金鱼"/>
  <!--Type page-->
  
      <meta property="og:type" content="article" />
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Title -->
  
  <title>数据分析师招聘信息爬取及分析 - 不会游泳的金鱼</title>


  <link rel="shortcut icon" href="https://raw.githubusercontent.com/JIANGWQ2017/PostImg/master/favicon.ico">

  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css">

  <!----------------------------
  https://github.com/GallenHu/hexo-theme-Daily

 _____            _   _
|  __ \          (_) | |
| |  | |   __ _   _  | |  _   _
| |  | |  / _` | | | | | | | | |
| |__| | | (_| | | | | | | |_| |
|_____/   \__,_| |_| |_|  \__, |
                          __/ |
                         |___/

    --------------------------->

<link rel="stylesheet" href="\assets\css\APlayer.min.css" class="aplayer-style-marker">
<script src="\assets\js\APlayer.min.js" class="aplayer-script-marker"></script>
</head>


<body>

  <!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="https://raw.githubusercontent.com/JIANGWQ2017/PostImg/master/icon.png" alt="不会游泳的金鱼" height="60">
        
      </a>
    </div>
    <!-- Navigation -->
    <nav class="navbar">
      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  home
                
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  archive
                
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            数据分析师招聘信息爬取及分析
            
          </h1>
          <p class="posted-on">
          2018-09-08
          </p>
          <div class="tags-links">
            
              
                <a href="/tags/Tech/" rel="tag">
                  Tech
                </a>
              
            
          </div>
        </div>
        <!-- Post Main Content -->
        <div class="entry-content has_line_number">
          <p>本文的源码托管于<a href="https://github.com/JIANGWQ2017/web-scraping/tree/master/zhaopin" target="_blank" rel="noopener">github</a></p>

        <div id="aplayer-YtuGUBMC" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">
            <pre class="aplayer-lrc-content"></pre>
        </div>
        <script>
          var ap = new APlayer({
            element: document.getElementById("aplayer-YtuGUBMC"),
            narrow: false,
            autoplay: true,
            showlrc: false,
            music: {
              title: "春、恋、花以外の",
              author: "栗川舜",
              url: "http://fs.open.kugou.com/2cf28f79132adbe745c22f8e2b1089b7/5b984386/G006/M04/09/03/poYBAFS-iR-AD_xRAC-ua3tIsbI654.mp3",
              pic: "",
              lrc: ""
            }
          });
          window.aplayers || (window.aplayers = []);
          window.aplayers.push(ap);
        </script>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a><font color="">Motivation</font></h1><p align="justify">数据分析在国内算是新兴职业，虽然网上关于数据分析师的信息很多，但是大多都不完整，不同层级的公司对数据分析师的定位也不同，导致对于数据分析师到底需要什么样的技能存在很大的混淆。故本文旨在找出数据分析师的技能范围，以及不同的技能对工资水平的影响。</p>

<hr>
<h1 id="Scraping"><a href="#Scraping" class="headerlink" title="Scraping"></a>Scraping</h1><p align="justify">  采用目前流行的scrapy爬虫框架爬取智联招聘，关键词为<em>数据分析师</em>，行业_不限<em>，月薪</em>不限<em>，范围为</em>全国_。虽然这个课题已经被无数人做过，网上也有很多教程，但是这确实是爬虫新手练手的一个好项目，值得一试。 </p>

<h3 id="Dynamic-loading"><a href="#Dynamic-loading" class="headerlink" title="Dynamic loading"></a>Dynamic loading</h3><p>在首页的关键字输入框中输入数据分析师，获得该请求的url<img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/url.png" alt="url"></p>
<p align="justify"> 如果直接在爬虫中请求这个url，会发现行不通，因为它不会加载智能匹配这一行以下的内容，之下的内容是动态加载的，也就是说不做任何处理的话是得不到任何工作信息的。</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/jobs.png" alt="jobs"></p>
<p>解决动态加载网站的方法主要有两种：</p>
<p align="justify"> 1： 利用selenium这个自动化测试工具，结合webdriver来模拟浏览器的行为，selenium对主流浏览器都有很好的支持，但是缺点便是这种方法会打开一个新的浏览器，对于大型爬虫来说，资源，特别是时间资源，消耗严重。</p>

<p align="justify"> 2： 找到动态加载请求的url,然后在爬虫中直接请求这个url。本文采用这种方法解决动态加载网站爬取的问题</p>

<p>在chrome浏览器中inspect模式中分析network的行为，找一找便可以找到’sou?pagesize=6…’.<br><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/job_source.png" alt="job_source"></p>
<p>得到它的request url，检验一下</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/json.png" alt="json"></p>
<p align="justify"> 发现受到的是一个json格式的response。包含所有我们想要的信息，但是并不是全部的信息，我们想要的岗位描述，技能要求，工作内容等信息都不在该json文件中，不过好消息是，至此，动态加载的问题得以解决。</p>

<p>目前用到的代码为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">name = <span class="string">'jobspider'</span></span><br><span class="line">allowed_domains = [<span class="string">'https://fe-api.zhaopin.com'</span>]</span><br><span class="line">start_urls = [<span class="string">'''https://fe-api.zhaopin.com/c/i/sou?pageSize=60&amp;cityId=489&amp;</span></span><br><span class="line"><span class="string">	workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;</span></span><br><span class="line"><span class="string">	jobWelfareTag=-1&amp;kw=%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88&amp;kt=3&amp;</span></span><br><span class="line"><span class="string">	lastUrlQuery=%7B%22jl%22:%22489%22,%22kw%22:%22%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88%22,%22kt%22:%223%22%7D'''</span>]</span><br><span class="line"><span class="comment"># url prefix and postfix to construct new url about job details</span></span><br><span class="line">url_prefix = <span class="string">'https://jobs.zhaopin.com/'</span></span><br><span class="line">url_surfix = <span class="string">'.htm'</span></span><br><span class="line"><span class="comment"># parameter needed to construct next main page url</span></span><br><span class="line">start_num = <span class="number">0</span></span><br><span class="line"><span class="comment"># count page num</span></span><br><span class="line">page = <span class="number">1</span></span><br><span class="line"><span class="comment"># count job num in one page</span></span><br><span class="line">count = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">	js = json.loads(response.body)</span><br><span class="line">	job_count = <span class="number">1</span></span><br><span class="line">	<span class="comment"># used as the stop condiction, if num_job_in_page = 0 , means no more jobs to scrape</span></span><br><span class="line">	num_job_in_page = len(js[<span class="string">'data'</span>][<span class="string">'results'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># fetch basic infos in a page</span></span><br><span class="line">	<span class="keyword">for</span> job <span class="keyword">in</span> js[<span class="string">'data'</span>][<span class="string">'results'</span>]:</span><br><span class="line">		<span class="comment"># needed to construction job detail page url</span></span><br><span class="line">		company_num = job[<span class="string">'number'</span>]</span><br><span class="line">		<span class="comment"># instantiate JobscrapingItem</span></span><br><span class="line">		item = JobscrapingItem()</span><br><span class="line">		<span class="comment"># extracting informations</span></span><br><span class="line">		item[<span class="string">'job_name'</span>] = job[<span class="string">'jobName'</span>],</span><br><span class="line">		item[<span class="string">'job_salary'</span>] = job[<span class="string">'salary'</span>],</span><br><span class="line">		item[<span class="string">'workExperienced'</span>] = job[<span class="string">'workingExp'</span>][<span class="string">'name'</span>],</span><br><span class="line">		item[<span class="string">'eduLevel'</span>] = job[<span class="string">'eduLevel'</span>][<span class="string">'name'</span>]</span><br><span class="line">		item[<span class="string">'job_empltype'</span>] = job[<span class="string">'emplType'</span>],</span><br><span class="line">		item[<span class="string">'job_welfare'</span>]= job[<span class="string">'welfare'</span>],</span><br><span class="line">		item[<span class="string">'job_type'</span>] = job[<span class="string">'jobType'</span>][<span class="string">'display'</span>]</span><br><span class="line">		item[<span class="string">'company_url'</span>] = job[<span class="string">'company'</span>][<span class="string">'url'</span>],</span><br><span class="line">		item[<span class="string">'company_name'</span>] = job[<span class="string">'company'</span>][<span class="string">'name'</span>],</span><br><span class="line">		item[<span class="string">'company_type'</span>] = job[<span class="string">'company'</span>][<span class="string">'type'</span>][<span class="string">'name'</span>],</span><br><span class="line">		item[<span class="string">'company_size'</span>] = job[<span class="string">'company'</span>][<span class="string">'size'</span>][<span class="string">'name'</span>],</span><br><span class="line">		item[<span class="string">'company_geo'</span>] = job[<span class="string">'geo'</span>],</span><br><span class="line">		item[<span class="string">'company_city'</span>] = job[<span class="string">'city'</span>][<span class="string">'display'</span>]</span><br><span class="line">		<span class="comment">#fecthing details</span></span><br><span class="line">		print(<span class="string">'fecthing details of job &#123;&#125; in page &#123;&#125;...'</span>.format(str(job_count),str(self.page)))</span><br><span class="line">		job_count += <span class="number">1</span></span><br><span class="line">		<span class="comment">#construct job detail page url</span></span><br><span class="line">		detail_url = self.url_prefix + str(company_num) + self.url_surfix</span><br><span class="line">		sleep(<span class="number">0.5</span>)</span><br><span class="line">		<span class="keyword">yield</span> scrapy.Request(detail_url,meta=&#123;<span class="string">'item'</span>:item&#125;,callback=self.job_details,dont_filter=<span class="keyword">True</span>)</span><br><span class="line">	print(<span class="string">'\n finished getting infos in page &#123;&#125;\n'</span>.format(str(self.page)))</span><br><span class="line">	self.page += <span class="number">1</span></span><br><span class="line">	<span class="comment"># iteration;  fetch next main page url, until no more jobs to scrape</span></span><br><span class="line">	prefix = self.start_urls[<span class="number">0</span>].split(<span class="string">'?'</span>)[<span class="number">0</span>]</span><br><span class="line">	surfix = self.start_urls[<span class="number">0</span>].split(<span class="string">'?'</span>)[<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">if</span> num_job_in_page != <span class="number">0</span>:</span><br><span class="line">		self.start_num += <span class="number">60</span></span><br><span class="line">		next_url = prefix+<span class="string">'?start='</span>+str(self.start_num)+<span class="string">'&amp;'</span> + surfix</span><br><span class="line">		<span class="keyword">yield</span> scrapy.Request(next_url,callback = self.parse,dont_filter=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="获得岗位信息"><a href="#获得岗位信息" class="headerlink" title="获得岗位信息"></a>获得岗位信息</h3><p align="justify"> 那么怎么得到细节信息呢？它们包含在岗位独有的网页中，那么首先随便进入一个公司的网页，url是类似这样的：<a href="https://jobs.zhaopin.com/455611616251174.htm，发现我们想要的都在里面。故我们在之前json文件内得到的company_num在这里派上用场。对于每一个公司，我们构造对应的url。请求这个url" target="_blank" rel="noopener">https://jobs.zhaopin.com/455611616251174.htm，发现我们想要的都在里面。故我们在之前json文件内得到的company_num在这里派上用场。对于每一个公司，我们构造对应的url。请求这个url</a>, 从response中得到岗位细节描述信息。现在对于每一个岗位的信息我们都可以获得了，之后利用简单的for循环就可以得到一个搜索页面中所有job的所有信息。</p>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">job_details</span><span class="params">(self,response)</span>:</span></span><br><span class="line">	item = response.meta[<span class="string">'item'</span>]</span><br><span class="line">	<span class="comment">#fetch description for each job</span></span><br><span class="line">	description_list = response.xpath(<span class="string">'/html/body/div[6]/div[1]/div[1]/div[1]/div[1]//text()'</span>).extract()</span><br><span class="line">	description = <span class="string">''</span></span><br><span class="line">	<span class="keyword">for</span> des <span class="keyword">in</span> description_list:</span><br><span class="line">		description += des.strip()</span><br><span class="line">	item[<span class="string">'description'</span>] = description</span><br><span class="line">	print(<span class="string">'\nadd one job &#123;&#125;\n'</span>.format(str(self.count)))</span><br><span class="line">	self.count += <span class="number">1</span></span><br><span class="line">	<span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p align="justify"> 但是还有一个问题，这只是基于第一页搜索结果的信息获取，怎么获得第二页岗位信息呢？答案也很简单，构造第二页岗位的url，然后去请求就好了。步骤如下： 先在浏览器中点击下一页的按钮，分析network中的活动，如下图：</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/next_rul.png" alt="next_url"></p>
<p align="justify"> 对比第一页搜索的url 和 第二页的url， 发现多了一段’start=60’，第三页是’start=120’,以此类推，每一个搜索结果包含60个结果，那结束条件是什么呢？如果start=xxx,xxx已经超过了所有jobs的数量，返回的json文件为空，就此就可以作为爬虫的结束条件。按照这个思路，就可以爬取所有岗位的所有信息了。</p>

<p align="justify"> 这里有一个注意的点，在scrapy.Request()函数中需要加上meta参数，具体是<code>meta={&#39;item&#39;:item}</code>，将item传递给指定的callback函数，为什么一定要这么做呢？ 因为scrapy的 scheduler组件在处理多个request的时候是并行的，并不是普通的code执行顺序一样串行进行的，按照普通code的顺序是爬取第一个main page(具体是第一个json)，第二步得到第一个main page中第一个job的job name,job salary, edulevel…等信息，构造岗位独有的url， 执行到<code>scrapy.Request(detail_url,meta={&#39;item&#39;:item},callback=self.job_details,dont_filter=True)</code>，调用callback函数去处理job description， 然后等待callback函数返回，得到返回值之后继续for循环，得到下一个job的所有信息，等到这个main page中所有jobs 处理完，构造下页招聘信息的url，直到爬取所有岗位信息。但是问题就出在，scrapy不是串行执行的，当遇到一个request，这个request不是马上执行，而是装进scheduler，至于什么是scheduler，参见<a href="https://doc.scrapy.org/en/0.9/topics/architecture.html" target="_blank" rel="noopener">doc</a>。接着顺序执行下一行代码，问题就来了，第一个item对象的信息就会在这个过程中被覆盖掉，等到第一个request开始执行的时候，item对象的信息已经不是第一个job的信息了，岗位描述和岗位的基础信息就对应不上了。 meta参数就是用来解决这个问题，它将当前item对象传递给callback函数，callback函数通过<code>item = response.meta[&#39;item&#39;]</code> 提取item对象。</p>

<hr>
<h3 id="item对象"><a href="#item对象" class="headerlink" title="item对象"></a>item对象</h3><p align="justify"> item对象类似字典，可以理解成一个包含数据的包裹，在pipeline.py里指定对item进行的操作，例如存入数据库。首先在items.py文件中定义item对象，field对象用于给item字段指定元数据。</p>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobscrapingItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">   		<span class="comment"># define the fields for your item here like:</span></span><br><span class="line">   		<span class="comment"># name = scrapy.Field()</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># initilize item object</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># job name</span></span><br><span class="line">	job_name = scrapy.Field()</span><br><span class="line">	<span class="comment"># job salary</span></span><br><span class="line">	job_salary = scrapy.Field()</span><br><span class="line">	<span class="comment"># required work experience</span></span><br><span class="line">	workExperienced = scrapy.Field()</span><br><span class="line">	<span class="comment"># required education level</span></span><br><span class="line">	eduLevel = scrapy.Field()</span><br><span class="line">	<span class="comment"># employment type </span></span><br><span class="line">	job_empltype = scrapy.Field()</span><br><span class="line">	<span class="comment">#welfare</span></span><br><span class="line">	job_welfare= scrapy.Field()</span><br><span class="line">	<span class="comment"># job category </span></span><br><span class="line">	job_type = scrapy.Field()</span><br><span class="line">	<span class="comment"># company url</span></span><br><span class="line">	company_url = scrapy.Field()</span><br><span class="line">	<span class="comment"># company name</span></span><br><span class="line">	company_name = scrapy.Field()</span><br><span class="line">	<span class="comment"># company category</span></span><br><span class="line">	company_type = scrapy.Field()</span><br><span class="line">	<span class="comment">#company size</span></span><br><span class="line">	company_size = scrapy.Field()</span><br><span class="line">	<span class="comment">#company geographic position</span></span><br><span class="line">	company_geo = scrapy.Field()</span><br><span class="line">	<span class="comment"># city that company locate </span></span><br><span class="line">	company_city = scrapy.Field()</span><br><span class="line">	<span class="comment">#job details description</span></span><br><span class="line">	description = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>然后按照第一部分代码中所展示的那样，和操作字典一样操作item对象。</p>
<hr>
<h3 id="存入数据库"><a href="#存入数据库" class="headerlink" title="存入数据库"></a>存入数据库</h3><p align="justify"> 这里我用的是mongodb，当然其他主流的数据库都是可以的，例如mysql，postgresql。使用方法是，修改pipeline文件：</p>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobscrapingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		client = MongoClient()</span><br><span class="line">		<span class="comment"># select database 'job_data', if not exist, creat one</span></span><br><span class="line">		self.db = client[<span class="string">'job_data'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">		<span class="comment"># insert data into collection :'data_related_jobs'</span></span><br><span class="line">		self.db.data_related_jobs.insert(dict(item))</span><br><span class="line">		<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h1><p align="justify"> 所有数据都已经存入了本地mongodb，大概有5600多条数据，为了后续处理起来比较方便，将数据库中的数据导出成csv格式文件，后续采用python处理csv文件十分的方便。在此之前先处理一下jupyter notebook对中文的支持问题。</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/font.png" alt="font"></p>
<p>import一些必要的库，load data之后就可以进行分析阶段最重要的工作了–data cleaning。</p>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p align="justify"> 工资的的数据格式大致有以下几种： 1： 8k-10k ； 2： 工资面议 3： 1千以下， 4：8.5k-10.5k。 对于第一种和第四种，我们使用正则表达式提取出max salary , min salary and mean salary三个features。对于第3种，最高工资，最低工资和平均工资都等于提取出的数字结果。至于第二种，最高，最低，平均工资都设置成nan值。</p> 

<p align="justify"> 公司福利： 智联招聘上的公司福利最多只能列上五个，而且从爬取到的数据来看，不同种类的福利只有有限项，也就是说工资welfare是一个categorical feature， 所以我们要做的就是将公司福利拆分成很多个哑变量，为后续机器学习部分打下基础，如果有的ML的话；如果不打算做ML，这一步可以省略。</p>

<p>城市： 将’北京-朝阳区’这种地名修改成’北京’。</p>
<p>工作类型： 将细分的工作类型归到最近的大类，减少类型数。</p>
<hr>
<h3 id="wordcloud"><a href="#wordcloud" class="headerlink" title="wordcloud"></a>wordcloud</h3><p align="justify"> 利用wordcloud，以及jieba库可以很好的实现中文的分词和生成词云，首先如果不进行任何操作，直接利用jieba库去做分词，然后生成的词云是这样的：</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/wc1.png" alt="wc1"></p>
<p>几乎没有任何有价值的信息，那我们就需要去去掉些没有意义的词，首先列举出前100个最重要的tags</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/top100.png" alt="tags"></p>
<p>观察这些tags，筛选出其中没有意义的词加入stopword，使得jieba库在分词是可以忽略这些词</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/stopword.png" alt="stopwords"></p>
<p>生成第二个词云</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/wc2.png" alt="wc2"></p>
<p align="justify"> 虽然效果比第一个好一点，但是还是没有什么实质用处。那么我们可以直接指定一些词汇，看看他们到底在多少岗位描述中提到过。在这里我自己定义了一组我个人感觉和数据分析岗位有关的技能</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/skills.png" alt="skills"></p>
<p align="justify"> 但是这个地方存在一个小问题，我们不能直接利用jieba来进行分词了，因为例如机器学习这种的组合词汇在jiaba库的分词下是会被分成机器和学习两个词汇。所以我们要采用其他方案，我使用的正则表达式，利用它去提取description中的关键词。</p>

<p align="justify"> 还有一个问题是’业务’这个关键词，在很多时候，懂得该行业的业务流程等会对数据分析有很多的帮助，但是该关键词的用途太为广泛，在这里我忽略了该关键词，不将它作为必要技能之一。生成第三个词云如下图：</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/wc3.png" alt="wc3"></p>
<p align="justify"> 我们可以很清晰的看出来，统计，数学，大数据， 数据库，以及建模是词频最高的几个关键词。统计和数学作为基础技能，出现率最高完全不出意料，大数据的出现一定程度上反应除了国内越来越多的公司开始重视大数据业务，普通的关联性和非关联数据库的解决方案已经满足不了越来越大的数据总量。但有些出乎意外是，根据笔者在加拿大留学的感觉，机器学习以及python在国外的计算机相关专业学生当中，已经几乎成为标配技能之一，说明国内在这一方面起步较晚，相关岗位数量有限，不过该类岗位通常门槛较高，数量少也在情理之中。</p>

<hr>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>首先来看看哪些城市的工作机会最多</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/job_number.png" alt="job_numpy"></p>
<p>北上广深果然不负众望，成都，南京，合肥紧追其后…</p>
<p>再来看看公司的规模分布</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/company_size.png" alt="company_size"></p>
<p>前五个城市中，中等规模的公司对数据分析师的需求最高，这个现象可能对job hunting 有些许帮助。</p>
<p>对学历的要求怎么样呢？</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/edu_level.png" alt="edulevel"></p>
<p align="justify"> 绝大多数只需要本科以上，甚至很大部分只要大专学历就可以…说明国内对数据分析师的定义还是有待提高，在办公室做excel，做ppt，月处理数据量3，5k条，那可不算数据分析师哦。</p>

<p>工作经历要求又是什么样呢？</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/work_exp.png" alt="workexp"></p>
<p>总体来说，门槛不高，对像笔者这种新入门的咸鱼来说还是挺和谐的。</p>
<p align="justify"> 终于来到了最关键的部分了，薪水部分！！！<br>为了排除部分城市职位很少，outlier支配平均工资的情况发生，这里过滤掉了50个职位以下的城市，按照平均工资递减的顺序排序结果：</p>

<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/mean_salary.png" alt="mean_salary"></p>
<p align="justify"> 北上广深以及杭州的平均工资都在万元以上，第6名开始就已经突降到8k多。所以各位有志于从事数据分析师的同学，知道该去那座城市了吧。</p>

<p>平均工资知道了，那我学什么技能可以提高工资？</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/corre.png" alt="corr"></p>
<p>额…office,数据可视化，办公软件，爬虫对工资是负相关的，意味着提到这些技能的工作薪水是偏低的。数学，建模，机器学习，算法是对工资正相关。学好数学，就成功了一半！！</p>
<p>下图显示的是提到每个技能工作的平均工资：</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/skill_salary.png" alt="skillsalary"></p>
<p>对比上面两图，matlab屌丝逆袭，成为平均工资最高的技能，大概是因为使用matlab通常是用来实现算法和进行数学计算，这类工作门槛相对较高。但是matlab与工资的相关系数不算突出，猜测是工资波动很大，下图来验证这个观点：</p>
<p><img src="https://raw.githubusercontent.com/JIANGWQ2017/MarkdownImg/master/skill_salary_std.png" alt="skill_std"></p>
<p>哈哈哈，观点得到验证正确！</p>
<hr>
<h1 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h1><p align="justify"> 本来打算利用DT, RF或者其他ML的模型来预测工资的，大致思路是给定工作的城市，类型，公司大小，福利，技能要求等信息，做回归模型得到一个回归工资，作为参考工资，如果实际工资低于参考工资，则申请该公司时可能需要提出提高工资的要求。现在还有些其他事情要处理，先在这里记一下，有时间回来补坑。</p>

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a href="https://www.xncoding.com/2016/03/16/scrapy-05.html" target="_blank" rel="noopener">https://www.xncoding.com/2016/03/16/scrapy-05.html</a></p>
<p>[2] <a href="http://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html" target="_blank" rel="noopener">http://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html</a></p>

        </div>
      </article>
    </div>
    <!-- Comments -->
    <div class="container">
      
<section id="comment">
  <!-- <h1 class="title">Comments</h1> -->

  
</section>


    </div>
    <!-- Pre or Next -->
    <div class="nav-links">
      
        <div class="nav-previous">
          <a href="/2018/09/10/building-blog/" rel="prev"><span class="meta-arraw meta-arraw-left"></span> Older Posts</a>
        </div>
      
      
        <div class="nav-next">
          <a href="/2018/09/08/lost/" rel="prev">Newer Posts <span class="meta-arraw meta-arraw-right"></span></a>
        </div>
      
    </div>

  </div>
</div>


  <!-- Footer -->
  <!-- Footer-widgets -->
<div class="footer-widgets">
  <div class="row inside-wrapper">
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">About</h1>
        <div class="custom-widget-content">
          
          <ul><li><a href="https://github.com/GallenHu/hexo-theme-Daily/wiki">About</a></li></ul>
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Contact</h1>
        <div class="widget-text">
          
            
              <a href="https://github.com/JIANGWQ2017" class="icon icon-github" target="_blank">github</a>
            
          
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Search</h1>
        <div class="widget-text">
          <form onSubmit="return appDaily.submitSearch('')">
            <p>
              <input type="text" placeholder="search..." id="homeSearchInput">
            </p>
            <!-- <input type="submit" value="GO"> -->
          </form>
        </div>
      </aside>
    </div>
  </div>
</div>
<!-- Footer -->
<footer class="site-info">
  <p>
    <span>不会游泳的金鱼 &copy; 2018</span>
    
      <span class="split">|</span>
      <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> with Theme <a href="https://github.com/GallenHu/hexo-theme-Daily" target="_blank">Daily</a></span>
    
  </p>
</footer>


  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>





</body>

</html>
